{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d753ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a521eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    '<br>Today is a rainy day',\n",
    "    'Today is a sunny day',\n",
    "    'Is it sunny Today?',\n",
    "    'I really enjoyed walking in the snow today'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e869fd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'today': 2, 'is': 3, 'a': 4, 'day': 5, 'sunny': 6, 'br': 7, 'rainy': 8, 'it': 9, 'i': 10, 'really': 11, 'enjoyed': 12, 'walking': 13, 'in': 14, 'the': 15, 'snow': 16}\n"
     ]
    }
   ],
   "source": [
    "# num_words here represent the number of tokens (number of distinct words)\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86ad424f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7, 2, 3, 4, 8, 5], [2, 3, 4, 6, 5], [3, 9, 6, 2], [10, 11, 12, 13, 14, 15, 16, 2]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e586fba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4, 1, 5], [1, 9, 1, 8, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    'Today is a snowy day',\n",
    "    'Will it be rainy tomorrow?'\n",
    "]\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "print(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35acdeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  7  2  3  4  8  5]\n",
      " [ 0  0  0  2  3  4  6  5]\n",
      " [ 0  0  0  0  3  9  6  2]\n",
      " [10 11 12 13 14 15 16  2]]\n"
     ]
    }
   ],
   "source": [
    "# Default is having tokens prepadded\n",
    "padded = pad_sequences(sequences)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d5ada2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  2  3  4  8  5  0  0]\n",
      " [ 2  3  4  6  5  0  0  0]\n",
      " [ 3  9  6  2  0  0  0  0]\n",
      " [10 11 12 13 14 15 16  2]]\n"
     ]
    }
   ],
   "source": [
    "# Trying out post padded\n",
    "padded_post = pad_sequences(sequences, padding='post')\n",
    "print(padded_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35f0472a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  2  3  4  8  5]\n",
      " [ 2  3  4  6  5  0]\n",
      " [ 3  9  6  2  0  0]\n",
      " [12 13 14 15 16  2]]\n"
     ]
    }
   ],
   "source": [
    "# Trying out max length (truncating from the front)\n",
    "max_padded = pad_sequences(sequences, padding='post', maxlen=6)\n",
    "print(max_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02ea9fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  2  3  4  8  5]\n",
      " [ 2  3  4  6  5  0]\n",
      " [ 3  9  6  2  0  0]\n",
      " [10 11 12 13 14 15]]\n"
     ]
    }
   ],
   "source": [
    "# Trying out max length (truncating from behind)\n",
    "max_padded_post = pad_sequences(sequences, padding='post', maxlen=6, truncating='post')\n",
    "print(max_padded_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b22dd9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'startswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m----> 2\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m sentences \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mget_text()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AIML\\lib\\site-packages\\bs4\\__init__.py:328\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    326\u001b[0m rejections \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    327\u001b[0m success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmarkup, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_encoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeclared_html_encoding,\n\u001b[0;32m    329\u001b[0m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontains_replacement_characters) \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m    330\u001b[0m      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mprepare_markup(\n\u001b[0;32m    331\u001b[0m          markup, from_encoding, exclude_encodings\u001b[38;5;241m=\u001b[39mexclude_encodings)):\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AIML\\lib\\site-packages\\bs4\\builder\\_lxml.py:182\u001b[0m, in \u001b[0;36mLXMLTreeBuilderForXML.prepare_markup\u001b[1;34m(self, markup, user_specified_encoding, exclude_encodings, document_declared_encoding)\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_instruction_class \u001b[38;5;241m=\u001b[39m ProcessingInstruction\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# We're in HTML mode, so if we're given XML, that's worth\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;66;03m# noting.\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m     \u001b[43mDetectsXMLParsedAsHTML\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn_if_markup_looks_like_xml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_instruction_class \u001b[38;5;241m=\u001b[39m XMLProcessingInstruction\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AIML\\lib\\site-packages\\bs4\\builder\\__init__.py:535\u001b[0m, in \u001b[0;36mDetectsXMLParsedAsHTML.warn_if_markup_looks_like_xml\u001b[1;34m(cls, markup)\u001b[0m\n\u001b[0;32m    531\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mXML_PREFIX\n\u001b[0;32m    532\u001b[0m     looks_like_html \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mLOOKS_LIKE_HTML\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (markup \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mmarkup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m(prefix)\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m looks_like_html\u001b[38;5;241m.\u001b[39msearch(markup[:\u001b[38;5;241m500\u001b[39m])\n\u001b[0;32m    537\u001b[0m ):\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_warn()\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'startswith'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1feb268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
